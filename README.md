# SimpleBench

<div align="center">

![SimpleBench Logo](https://img.shields.io/badge/SimpleBench-Benchmark%20for%20LLMs-blue)
[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

SimpleBench est un outil l√©ger et efficace pour √©valuer les performances des mod√®les de langage (LLMs) ex√©cut√©s via [Ollama](https://ollama.com/). Il permet aux chercheurs et d√©veloppeurs de comparer objectivement diff√©rents mod√®les sur des t√¢ches standardis√©es de programmation et de raisonnement.

## ‚ú® Caract√©ristiques

- üöÄ **Simple d'utilisation** - Interface en ligne de commande intuitive
- üîÑ **Support multi-datasets** - Compatible avec HumanEval, CruxEval et Code-X-GLUE
- üìä **√âvaluation avanc√©e** - Tol√®re les diff√©rences de formatage, d'indentation et d'√©quivalence fonctionnelle
- üìà **Analyse d√©taill√©e** - Scripts d'analyse des performances inclus
- üß© **Extensible** - Facile √† adapter pour diff√©rents types de benchmarks

## üõ†Ô∏è Pr√©requis

- Python 3.13.5 ou sup√©rieur
- [Ollama](https://ollama.com/) install√© et en cours d'ex√©cution

## üì¶ Installation

1. Clonez le d√©p√¥t :
   ```bash
   git clone https://github.com/simple-bench/simple-bench.git
   cd simple-bench
   ```

2. Cr√©ez un environnement virtuel et installez les d√©pendances :
   ```bash
   python -m venv venv
   source venv/bin/activate  # Sur Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

## üöÄ Ex√©cution du benchmark

1. Assurez-vous qu'Ollama est en cours d'ex√©cution.

2. T√©l√©chargez le mod√®le que vous souhaitez tester :
   ```bash
   ollama pull qwen3:14b
   ```

3. Ex√©cutez le benchmark avec un dataset au choix :
   ```bash
   # Pour HumanEval
   python run_benchmark_final.py --model_name=qwen3:14b --dataset_source=sql-console-for-openai-openai-humaneval.json
   
   # Pour CruxEval
   python run_benchmark_final.py --model_name=qwen3:14b --dataset_source=sql-console-for-cruxeval-org-cruxeval.json
   
   # Pour Code-X-GLUE
   python run_benchmark_final.py --model_name=qwen3:14b --dataset_source=sql-console-for-google-code-x-glue-ct-code-to-text.json
   ```

### Options disponibles

| Option | Description | Valeur par d√©faut |
|--------|-------------|-------------------|
| `--model_name` | Nom du mod√®le Ollama √† tester | qwen3:14b |
| `--dataset_source` | Chemin vers le fichier source du dataset | ./sql-console-for-openai-openai-humaneval.json |
| `--dataset_type` | Type de dataset (humaneval, cruxeval, code_x_glue) | auto-d√©tect√© |
| `--num_responses` | Nombre de r√©ponses pour le vote majoritaire | 1 |
| `--temp` | Temp√©rature pour le mod√®le | 0.7 |
| `--max_tokens` | Nombre maximum de tokens √† g√©n√©rer | 2048 |
| `--top_p` | Valeur top_p pour le mod√®le | 0.95 |
| `--max_retries` | Nombre maximum de tentatives en cas d'erreur | 3 |
| `--custom_system_prompt` | Prompt syst√®me personnalis√© | prompt par d√©faut selon le dataset |

## üìä Analyse des r√©sultats

Le script `run_benchmark_final.py` affiche automatiquement les r√©sultats d√©taill√©s de l'√©valuation, notamment :

- Le nombre total de questions √©valu√©es
- Le nombre et pourcentage de r√©ponses correctes
- La r√©partition des r√©ponses correctes par m√©thode d'√©valuation :
  - Normalisation basique (correspondance exacte apr√®s nettoyage)
  - Normalisation avanc√©e (gestion de l'indentation)
  - Normalisation extr√™me (suppression des espaces et sauts de ligne)
  - Comparaison AST (analyse de la structure syntaxique)
  - √âquivalence IA (d√©tection d'√©quivalence fonctionnelle)

Ces statistiques d√©taill√©es vous permettent de comprendre pr√©cis√©ment les performances du mod√®le et les types de r√©ponses qu'il g√©n√®re.

## üß© Datasets support√©s

SimpleBench supporte nativement plusieurs datasets populaires pour l'√©valuation des mod√®les de langage :

### HumanEval

[HumanEval](https://huggingface.co/datasets/openai/openai_humaneval) est un benchmark d'OpenAI pour √©valuer les capacit√©s de g√©n√©ration de code. Il contient des probl√®mes de programmation Python avec des solutions et des tests.

### CruxEval

[CruxEval](https://huggingface.co/datasets/cruxeval-org/cruxeval) est un ensemble de probl√®mes de programmation con√ßu pour √©valuer les capacit√©s de raisonnement des mod√®les de langage.

### Code-X-GLUE

[CodeXGLUE](https://huggingface.co/datasets/google/code_x_glue_ct_code_to_text) est un benchmark de Google pour diverses t√¢ches li√©es au code, notamment la g√©n√©ration de descriptions √† partir de code source.

## üîß D√©tection automatique des datasets

SimpleBench d√©tecte automatiquement le type de dataset en fonction du nom du fichier ou de son contenu. Vous pouvez √©galement sp√©cifier explicitement le type avec l'option `--dataset_type`.

## üíØ √âvaluation avanc√©e

SimpleBench utilise une approche d'√©valuation avanc√©e qui combine plusieurs m√©thodes pour d√©tecter les r√©ponses correctes :

1. **Normalisation basique** : Supprime les balises et normalise les espaces
2. **Normalisation avanc√©e** : G√®re intelligemment l'indentation et les sauts de ligne
3. **Normalisation extr√™me** : Supprime tous les espaces et sauts de ligne pour d√©tecter les r√©ponses qui diff√®rent uniquement par le formatage
4. **Comparaison AST** : Analyse la structure syntaxique du code pour d√©tecter les √©quivalences structurelles
5. **√âvaluation par IA** : Utilise un mod√®le de langage pour d√©tecter les √©quivalences fonctionnelles

Cette approche permet de d√©tecter beaucoup plus pr√©cis√©ment les r√©ponses correctes, m√™me lorsqu'elles diff√®rent de la solution attendue en termes de style, de noms de variables ou d'approche algorithmique.

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou √† soumettre une pull request.

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.