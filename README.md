# SimpleBench

<div align="center">

![SimpleBench Logo](https://img.shields.io/badge/SimpleBench-Benchmark%20for%20LLMs-blue)
[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

SimpleBench est un outil l√©ger et efficace pour √©valuer les performances des mod√®les de langage (LLMs) ex√©cut√©s via [Ollama](https://ollama.com/). Il permet aux chercheurs et d√©veloppeurs de comparer objectivement diff√©rents mod√®les sur des t√¢ches standardis√©es de programmation et de raisonnement.

## ‚ú® Caract√©ristiques

- üöÄ **Simple d'utilisation** - Interface en ligne de commande intuitive
- üîÑ **D√©tection automatique des mod√®les** - Pas besoin de configuration manuelle
- üìä **√âvaluation robuste** - Tol√®re les diff√©rences de formatage dans les r√©ponses
- üìà **Analyse d√©taill√©e** - Scripts d'analyse des performances inclus
- üß© **Extensible** - Facile √† adapter pour diff√©rents types de benchmarks

## üõ†Ô∏è Pr√©requis

- Python 3.13.5 ou sup√©rieur
- [Ollama](https://ollama.com/) install√© et en cours d'ex√©cution

## üì¶ Installation

1. Clonez le d√©p√¥t :
   ```bash
   git clone https://github.com/simple-bench/simple-bench.git
   cd simple-bench
   ```

2. Cr√©ez un environnement virtuel et installez les d√©pendances :
   ```bash
   python -m venv venv
   source venv/bin/activate  # Sur Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

## üöÄ Ex√©cution du benchmark

1. Assurez-vous qu'Ollama est en cours d'ex√©cution.

2. T√©l√©chargez le mod√®le que vous souhaitez tester :
   ```bash
   ollama pull qwen3:14b
   ```

3. Ex√©cutez le benchmark :
   ```bash
   python run_benchmark_new.py --model_name=qwen3:14b --dataset_path=converted_code_bench.json
   ```

### Options disponibles

| Option | Description | Valeur par d√©faut |
|--------|-------------|-------------------|
| `--model_name` | Nom du mod√®le Ollama √† tester | qwen3:14b |
| `--dataset_path` | Chemin vers le fichier JSON du dataset | ./converted_code_bench.json |
| `--num_responses` | Nombre de r√©ponses pour le vote majoritaire | 1 |
| `--temp` | Temp√©rature pour le mod√®le | 0.7 |
| `--max_tokens` | Nombre maximum de tokens √† g√©n√©rer | 2048 |
| `--top_p` | Valeur top_p pour le mod√®le | 0.95 |
| `--max_retries` | Nombre maximum de tentatives en cas d'erreur | 3 |

## üìä Analyse des r√©sultats

Apr√®s avoir ex√©cut√© le benchmark, vous pouvez analyser les r√©sultats avec les scripts fournis :

### Statistiques g√©n√©rales

```bash
python analyze_results.py weave_export_simple_bench_[date].jsonl
```

Ce script affiche :
- Le nombre total de questions
- Le nombre et pourcentage de r√©ponses correctes
- Le nombre et pourcentage de r√©ponses incorrectes
- Le nombre et pourcentage de cas o√π la limite du mod√®le a √©t√© atteinte

### V√©rification d√©taill√©e des r√©ponses

```bash
python verify_responses.py weave_export_simple_bench_[date].jsonl
```

Ce script affiche une comparaison d√©taill√©e entre les r√©ponses du mod√®le et les r√©ponses attendues pour chaque question.

## üß© Ajout de nouveaux mod√®les

Le script d√©tecte automatiquement les nouveaux mod√®les Ollama, il n'est donc plus n√©cessaire de modifier manuellement le code. Il suffit de sp√©cifier le nom du mod√®le dans la commande :

```bash
python run_benchmark_new.py --model_name=nom-du-modele --dataset_path=converted_code_bench.json
```

Par exemple, pour tester le mod√®le `llama3:8b` :

```bash
ollama pull llama3:8b
python run_benchmark_new.py --model_name=llama3:8b --dataset_path=converted_code_bench.json
```

Le mod√®le sera automatiquement pr√©fix√© avec "ollama/" et configur√© lors de l'ex√©cution.

## üìù Format du dataset

### Source des questions

Les questions utilis√©es dans ce benchmark proviennent du dataset [CruxEval](https://huggingface.co/datasets/cruxeval-org/cruxeval), un ensemble de probl√®mes de programmation con√ßu pour √©valuer les capacit√©s de raisonnement des mod√®les de langage.

### Structure du dataset

Les datasets doivent √™tre au format JSON avec la structure suivante :

```json
{
  "eval_data": [
    {
      "question_id": 0,
      "prompt": "Question...",
      "answer": "R√©ponse attendue"
    },
    ...
  ]
}
```

### Conversion de datasets

Le script `convert_dataset.py` permet de convertir les donn√©es du format JSONL original de CruxEval vers le format JSON utilis√© par SimpleBench :

```bash
python convert_dataset.py
```

Ce script :
- Lit le fichier JSONL source (`test.jsonl_copy.txt`)
- Extrait les questions, entr√©es et sorties attendues
- Formate les prompts en fran√ßais
- G√©n√®re un fichier JSON compatible (`converted_code_bench.json`)

### Datasets additionnels √† tester

SimpleBench peut √™tre adapt√© pour fonctionner avec d'autres datasets de r√©f√©rence pour l'√©valuation des mod√®les de langage sur des t√¢ches de programmation :

- [HumanEval](https://huggingface.co/datasets/openai/openai_humaneval) - Un benchmark d'OpenAI pour √©valuer les capacit√©s de g√©n√©ration de code
- [CodeXGLUE](https://huggingface.co/datasets/google/code_x_glue_ct_code_to_text) - Un benchmark de Google pour diverses t√¢ches li√©es au code

Pour utiliser ces datasets, il faudra adapter le script de conversion pour transformer leurs formats sp√©cifiques au format JSON attendu par SimpleBench.

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou √† soumettre une pull request.

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.