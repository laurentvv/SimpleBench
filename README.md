# Bench AI with Ollama/LMStudio

<div align="center">

![Bench AI Logo](https://img.shields.io/badge/Bench%20AI-LLM%20Benchmark%20Tool-blue)
[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

Bench AI est un outil l√©ger et efficace pour √©valuer les performances des mod√®les de langage (LLMs) ex√©cut√©s via [Ollama](https://ollama.com/) et LMStudio. Il permet aux chercheurs et d√©veloppeurs de comparer objectivement diff√©rents mod√®les sur des t√¢ches standardis√©es de programmation et de raisonnement.

## üéØ R√©sultats R√©cents

- **Pr√©cision d'√©valuation** : 63% de r√©ponses correctes d√©tect√©es sur HumanEval
- **M√©thodes d'√©valuation** : 5 approches compl√©mentaires (normalisation, AST, IA)
- **Reproductibilit√©** : R√©sultats coh√©rents entre les ex√©cutions

## ‚ú® Caract√©ristiques

- üöÄ **Simple d'utilisation** - Interface en ligne de commande intuitive
- üîÑ **Support multi-datasets** - Compatible avec HumanEval, CruxEval et Code-X-GLUE
- üìä **√âvaluation avanc√©e** - Tol√®re les diff√©rences de formatage, d'indentation et d'√©quivalence fonctionnelle
- üìà **Analyse d√©taill√©e** - Scripts d'analyse des performances inclus
- üß© **Extensible** - Facile √† adapter pour diff√©rents types de benchmarks

## üõ†Ô∏è Pr√©requis

- Python 3.13.5 ou sup√©rieur
- [Ollama](https://ollama.com/) install√© et en cours d'ex√©cution

## üì¶ Installation

1. Clonez le d√©p√¥t :
   ```bash
   git clone https://github.com/laurentvv/bench-ai-ollama-lmstudio.git
   cd bench-ai-ollama-lmstudio
   ```

2. Cr√©ez un environnement virtuel et installez les d√©pendances :
   ```bash
   python -m venv venv
   source venv/bin/activate  # Sur Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

## üöÄ Ex√©cution du benchmark

1. Assurez-vous qu'Ollama est en cours d'ex√©cution.

2. T√©l√©chargez le mod√®le que vous souhaitez tester :
   ```bash
   ollama pull qwen3:14b
   ```

3. Ex√©cutez le benchmark avec un dataset au choix :
   ```bash
   # Pour HumanEval
   python run_benchmark_production.py --model_name=qwen3:14b --dataset_source=sql-console-for-openai-openai-humaneval.json
   
   # Pour CruxEval
   python run_benchmark_production.py --model_name=qwen3:14b --dataset_source=sql-console-for-cruxeval-org-cruxeval.json
   
   # Pour Code-X-GLUE
   python run_benchmark_production.py --model_name=qwen3:14b --dataset_source=sql-console-for-google-code-x-glue-ct-code-to-text.json
   ```

## Œπœá Suivi avec Weave

Bench AI est int√©gr√© avec [Weave](https://wandb.ai/site/weave), un outil puissant pour le suivi et la visualisation de vos exp√©riences.

### Configuration de Weave

1. **Cr√©ez un compte Weave** : Rendez-vous sur le [site de Weave](https://wandb.ai/site/weave) et cr√©ez un compte gratuit.

2. **Connectez-vous √† Weave** : Une fois votre compte cr√©√©, vous pouvez vous connecter via le CLI (non n√©cessaire pour ce projet, g√©r√© par l'API).

### Utilisation de Weave avec SimpleBench

Pour activer le suivi avec Weave, utilisez les options `--entity` et `--project` lors de l'ex√©cution du benchmark :

```bash
python run_benchmark_production.py \
  --model_name=qwen3:14b \
  --dataset_source=sql-console-for-openai-openai-humaneval.json \
  --entity="votre-entite-weave" \
  --project="nom-du-projet"
```

- `--entity` : Votre nom d'utilisateur ou d'organisation Weave.
- `--project` : Le nom du projet sous lequel vous souhaitez enregistrer l'exp√©rience.

Les r√©sultats de l'√©valuation, y compris les scores d√©taill√©s et les pr√©dictions du mod√®le, seront automatiquement envoy√©s √† votre projet Weave, vous permettant de :

- Comparer les performances de diff√©rents mod√®les
- Analyser les erreurs de pr√©diction
- Partager vos r√©sultats avec votre √©quipe

### Options disponibles

| Option | Description | Valeur par d√©faut |
|--------|-------------|-------------------|
| `--model_name` | Nom du mod√®le Ollama √† tester | qwen3:14b |
| `--dataset_source` | Chemin vers le fichier source du dataset | ./sql-console-for-openai-openai-humaneval.json |
| `--dataset_type` | Type de dataset (humaneval, cruxeval, code_x_glue) | auto-d√©tect√© |
| `--num_responses` | Nombre de r√©ponses pour le vote majoritaire | 1 |
| `--temp` | Temp√©rature pour le mod√®le | 0.1 |
| `--max_tokens` | Nombre maximum de tokens √† g√©n√©rer | 2048 |
| `--top_p` | Valeur top_p pour le mod√®le | 0.95 |
| `--max_retries` | Nombre maximum de tentatives en cas d'erreur | 3 |
| `--custom_system_prompt` | Prompt syst√®me personnalis√© | prompt par d√©faut selon le dataset |

## üìä Analyse des r√©sultats

Le script `run_benchmark_production.py` affiche automatiquement les r√©sultats d√©taill√©s de l'√©valuation, notamment :

- Le nombre total de questions √©valu√©es
- Le nombre et pourcentage de r√©ponses correctes
- Le temps d'ex√©cution total
- La r√©partition des r√©ponses correctes par m√©thode d'√©valuation :
  - **Normalisation basique** : Correspondance exacte apr√®s nettoyage
  - **Normalisation avanc√©e** : Gestion intelligente de l'indentation
  - **Normalisation extr√™me** : Suppression des espaces et sauts de ligne
  - **Comparaison AST** : Analyse de la structure syntaxique
  - **√âquivalence IA** : D√©tection d'√©quivalence fonctionnelle

Ces statistiques d√©taill√©es vous permettent de comprendre pr√©cis√©ment les performances du mod√®le et les types de r√©ponses qu'il g√©n√®re.

## üß© Datasets support√©s

Bench AI supporte nativement plusieurs datasets populaires pour l'√©valuation des mod√®les de langage :

### üìÅ Datasets inclus dans ce d√©p√¥t

Ce d√©p√¥t contient **3 fichiers de datasets** avec **30 questions chacun** :

- **`sql-console-for-openai-openai-humaneval.json`** - HumanEval (30 questions)
- **`sql-console-for-cruxeval-org-cruxeval.json`** - CruxEval (30 questions)  
- **`sql-console-for-google-code-x-glue-ct-code-to-text.json`** - Code-X-GLUE (30 questions)

> üí° **Pour plus de questions**, consultez les datasets complets sur Hugging Face (liens ci-dessous)

### HumanEval

[HumanEval](https://huggingface.co/datasets/openai/openai_humaneval) est un benchmark d'OpenAI pour √©valuer les capacit√©s de g√©n√©ration de code. Il contient des probl√®mes de programmation Python avec des solutions et des tests.

### CruxEval

[CruxEval](https://huggingface.co/datasets/cruxeval-org/cruxeval) est un ensemble de probl√®mes de programmation con√ßu pour √©valuer les capacit√©s de raisonnement des mod√®les de langage.

### Code-X-GLUE

[CodeXGLUE](https://huggingface.co/datasets/google/code_x_glue_ct_code_to_text) est un benchmark de Google pour diverses t√¢ches li√©es au code, notamment la g√©n√©ration de descriptions √† partir de code source.

## üîß D√©tection automatique des datasets

Bench AI d√©tecte automatiquement le type de dataset en fonction du nom du fichier ou de son contenu. Vous pouvez √©galement sp√©cifier explicitement le type avec l'option `--dataset_type`.

## üíØ √âvaluation avanc√©e

Bench AI utilise une approche d'√©valuation avanc√©e qui combine plusieurs m√©thodes pour d√©tecter les r√©ponses correctes :

1. **Normalisation basique** : Supprime les balises et normalise les espaces
2. **Normalisation avanc√©e** : G√®re intelligemment l'indentation et les sauts de ligne
3. **Normalisation extr√™me** : Supprime tous les espaces et sauts de ligne pour d√©tecter les r√©ponses qui diff√®rent uniquement par le formatage
4. **Comparaison AST** : Analyse la structure syntaxique du code pour d√©tecter les √©quivalences structurelles
5. **√âvaluation par IA** : Utilise un mod√®le de langage pour d√©tecter les √©quivalences fonctionnelles

Cette approche permet de d√©tecter beaucoup plus pr√©cis√©ment les r√©ponses correctes, m√™me lorsqu'elles diff√®rent de la solution attendue en termes de style, de noms de variables ou d'approche algorithmique.

## üöÄ Prochaines Fonctionnalit√©s

- Support de LMStudio pour les mod√®les locaux
- Dataset MBPP (Mostly Basic Python Problems)
- Interface web pour visualiser les r√©sultats
- Comparaison automatique entre mod√®les

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou √† soumettre une pull request.

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.