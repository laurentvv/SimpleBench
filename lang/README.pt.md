# Bench AI with Ollama/LMStudio

<div align="center">

[![English](https://img.shields.io/badge/lang-en-blue.svg)](../README.md) [![ä¸­æ–‡](https://img.shields.io/badge/lang-zh-blue.svg)](README.zh.md) [![à¤¹à¤¿à¤‚à¤¦à¥€](https://img.shields.io/badge/lang-hi-blue.svg)](README.hi.md) [![EspaÃ±ol](https://img.shields.io/badge/lang-es-blue.svg)](README.es.md) [![FranÃ§ais](https://img.shields.io/badge/lang-fr-blue.svg)](README.fr.md) [![Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://img.shields.io/badge/lang-ar-blue.svg)](README.ar.md) [![à¦¬à¦¾à¦‚à¦²à¦¾](https://img.shields.io/badge/lang-bn-blue.svg)](README.bn.md) [![Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://img.shields.io/badge/lang-ru-blue.svg)](README.ru.md) [![PortuguÃªs](https://img.shields.io/badge/lang-pt-blue.svg)](README.pt.md) [![Bahasa Indonesia](https://img.shields.io/badge/lang-id-blue.svg)](README.id.md)

![Bench AI Logo](https://img.shields.io/badge/Bench%20AI-LLM%20Benchmark%20Tool-blue)
[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

Bench AI Ã© uma ferramenta leve e eficiente para avaliar o desempenho de modelos de linguagem (LLMs) executados via [Ollama](https://ollama.com/) e LMStudio. Permite que pesquisadores e desenvolvedores comparem objetivamente diferentes modelos em tarefas padronizadas de programaÃ§Ã£o e raciocÃ­nio.

## ğŸ¯ Resultados Recentes

- **PrecisÃ£o de avaliaÃ§Ã£o**: 63% de respostas corretas detectadas no HumanEval
- **MÃ©todos de avaliaÃ§Ã£o**: 5 abordagens complementares (normalizaÃ§Ã£o, AST, IA)
- **Reprodutibilidade**: Resultados consistentes entre execuÃ§Ãµes

## âœ¨ CaracterÃ­sticas

- ğŸš€ **FÃ¡cil de usar** - Interface de linha de comando intuitiva
- ğŸ”„ **Suporte multi-dataset** - CompatÃ­vel com HumanEval, CruxEval e Code-X-GLUE
- ğŸ“Š **AvaliaÃ§Ã£o avanÃ§ada** - Tolera diferenÃ§as de formataÃ§Ã£o, indentaÃ§Ã£o e equivalÃªncia funcional
- ğŸ“ˆ **AnÃ¡lise detalhada** - Scripts de anÃ¡lise de desempenho incluÃ­dos
- ğŸ§© **ExtensÃ­vel** - FÃ¡cil de adaptar para diferentes tipos de benchmarks

## ğŸ› ï¸ PrÃ©-requisitos

- Python 3.13.5 ou superior
- [Ollama](https://ollama.com/) instalado e em execuÃ§Ã£o

## ğŸ“¦ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
   ```bash
   git clone https://github.com/laurentvv/bench-ai-ollama-lmstudio.git
   cd bench-ai-ollama-lmstudio
   ```

2. Crie um ambiente virtual e instale as dependÃªncias:
   ```bash
   python -m venv venv
   source venv/bin/activate  # No Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

## ğŸš€ Executando o benchmark

1. Certifique-se de que o Ollama estÃ¡ em execuÃ§Ã£o.

2. Baixe o modelo que vocÃª quer testar:
   ```bash
   ollama pull qwen3:14b
   ```

3. Execute o benchmark com um dataset de sua escolha:
   ```bash
   # Para HumanEval
   python run_benchmark_production.py --model_name=qwen3:14b --dataset_source=sql-console-for-openai-openai-humaneval.json
   ```

## ğŸ“Š Rastreamento com Weave

Bench AI estÃ¡ integrado com [Weave](https://wandb.ai/site/weave), uma ferramenta poderosa para rastrear e visualizar seus experimentos.

## ğŸ§© Datasets Suportados

Este repositÃ³rio contÃ©m **3 arquivos de datasets** com **30 perguntas cada**:

- **`sql-console-for-openai-openai-humaneval.json`** - HumanEval (30 perguntas)
- **`sql-console-for-cruxeval-org-cruxeval.json`** - CruxEval (30 perguntas)
- **`sql-console-for-google-code-x-glue-ct-code-to-text.json`** - Code-X-GLUE (30 perguntas)

## ğŸ’¯ AvaliaÃ§Ã£o AvanÃ§ada

Bench AI usa uma abordagem de avaliaÃ§Ã£o avanÃ§ada que combina mÃºltiplos mÃ©todos para detectar respostas corretas:

1. **NormalizaÃ§Ã£o bÃ¡sica**: Remove tags e normaliza espaÃ§os
2. **NormalizaÃ§Ã£o avanÃ§ada**: Lida inteligentemente com indentaÃ§Ã£o e quebras de linha
3. **NormalizaÃ§Ã£o extrema**: Remove todos os espaÃ§os e quebras de linha
4. **ComparaÃ§Ã£o AST**: Analisa a estrutura sintÃ¡tica do cÃ³digo
5. **AvaliaÃ§Ã£o por IA**: Usa um modelo de linguagem para detectar equivalÃªncia funcional

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir uma issue ou enviar um pull request.

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para detalhes.