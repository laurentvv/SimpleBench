# Bench AI with Ollama/LMStudio

<div align="center">

[![English](https://img.shields.io/badge/lang-en-blue.svg)](../README.md) [![‰∏≠Êñá](https://img.shields.io/badge/lang-zh-blue.svg)](README.zh.md) [![‡§π‡§ø‡§Ç‡§¶‡•Ä](https://img.shields.io/badge/lang-hi-blue.svg)](README.hi.md) [![Espa√±ol](https://img.shields.io/badge/lang-es-blue.svg)](README.es.md) [![Fran√ßais](https://img.shields.io/badge/lang-fr-blue.svg)](README.fr.md) [![ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://img.shields.io/badge/lang-ar-blue.svg)](README.ar.md) [![‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ](https://img.shields.io/badge/lang-bn-blue.svg)](README.bn.md) [![–†—É—Å—Å–∫–∏–π](https://img.shields.io/badge/lang-ru-blue.svg)](README.ru.md) [![Portugu√™s](https://img.shields.io/badge/lang-pt-blue.svg)](README.pt.md) [![Bahasa Indonesia](https://img.shields.io/badge/lang-id-blue.svg)](README.id.md)

![Bench AI Logo](https://img.shields.io/badge/Bench%20AI-LLM%20Benchmark%20Tool-blue)
[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

Bench AI es una herramienta ligera y eficiente para evaluar el rendimiento de modelos de lenguaje (LLMs) ejecutados a trav√©s de [Ollama](https://ollama.com/) y LMStudio. Permite a investigadores y desarrolladores comparar objetivamente diferentes modelos en tareas estandarizadas de programaci√≥n y razonamiento.

## üéØ Resultados Recientes

- **Precisi√≥n de evaluaci√≥n**: 63% de respuestas correctas detectadas en HumanEval
- **M√©todos de evaluaci√≥n**: 5 enfoques complementarios (normalizaci√≥n, AST, IA)
- **Reproducibilidad**: Resultados consistentes entre ejecuciones

## ‚ú® Caracter√≠sticas

- üöÄ **F√°cil de usar** - Interfaz de l√≠nea de comandos intuitiva
- üîÑ **Soporte multi-dataset** - Compatible con HumanEval, CruxEval y Code-X-GLUE
- üìä **Evaluaci√≥n avanzada** - Tolera diferencias de formato, indentaci√≥n y equivalencia funcional
- üìà **An√°lisis detallado** - Scripts de an√°lisis de rendimiento incluidos
- üß© **Extensible** - F√°cil de adaptar para diferentes tipos de benchmarks

## üõ†Ô∏è Prerrequisitos

- Python 3.13.5 o superior
- [Ollama](https://ollama.com/) instalado y ejecut√°ndose

## üì¶ Instalaci√≥n

1. Clona el repositorio:
   ```bash
   git clone https://github.com/laurentvv/bench-ai-ollama-lmstudio.git
   cd bench-ai-ollama-lmstudio
   ```

2. Crea un entorno virtual e instala las dependencias:
   ```bash
   python -m venv venv
   source venv/bin/activate  # En Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

## üöÄ Ejecutar el benchmark

1. Aseg√∫rate de que Ollama est√© ejecut√°ndose.

2. Descarga el modelo que quieres probar:
   ```bash
   ollama pull qwen3:14b
   ```

3. Ejecuta el benchmark con un dataset de tu elecci√≥n:
   ```bash
   # Para HumanEval
   python run_benchmark_production.py --model_name=qwen3:14b --dataset_source=sql-console-for-openai-openai-humaneval.json
   
   # Para CruxEval
   python run_benchmark_production.py --model_name=qwen3:14b --dataset_source=sql-console-for-cruxeval-org-cruxeval.json
   
   # Para Code-X-GLUE
   python run_benchmark_production.py --model_name=qwen3:14b --dataset_source=sql-console-for-google-code-x-glue-ct-code-to-text.json
   ```

## üìä Seguimiento con Weave

Bench AI est√° integrado con [Weave](https://wandb.ai/site/weave), una herramienta poderosa para rastrear y visualizar tus experimentos.

### Configuraci√≥n de Weave

Para habilitar el seguimiento con Weave, usa las opciones `--entity` y `--project` al ejecutar el benchmark:

```bash
python run_benchmark_production.py \
  --model_name=qwen3:14b \
  --dataset_source=sql-console-for-openai-openai-humaneval.json \
  --entity="tu-entidad-weave" \
  --project="nombre-proyecto"
```

## üß© Datasets Soportados

Este repositorio contiene **3 archivos de datasets** con **30 preguntas cada uno**:

- **`sql-console-for-openai-openai-humaneval.json`** - HumanEval (30 preguntas)
- **`sql-console-for-cruxeval-org-cruxeval.json`** - CruxEval (30 preguntas)
- **`sql-console-for-google-code-x-glue-ct-code-to-text.json`** - Code-X-GLUE (30 preguntas)

## üíØ Evaluaci√≥n Avanzada

Bench AI utiliza un enfoque de evaluaci√≥n avanzado que combina m√∫ltiples m√©todos para detectar respuestas correctas:

1. **Normalizaci√≥n b√°sica**: Elimina etiquetas y normaliza espacios
2. **Normalizaci√≥n avanzada**: Maneja inteligentemente la indentaci√≥n y saltos de l√≠nea
3. **Normalizaci√≥n extrema**: Elimina todos los espacios y saltos de l√≠nea
4. **Comparaci√≥n AST**: Analiza la estructura sint√°ctica del c√≥digo
5. **Evaluaci√≥n por IA**: Usa un modelo de lenguaje para detectar equivalencias funcionales

## ü§ù Contribuir

¬°Las contribuciones son bienvenidas! No dudes en abrir un issue o enviar un pull request.

## üìÑ Licencia

Este proyecto est√° licenciado bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para detalles.